{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n6frT7pO5Q6h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6frT7pO5Q6h",
    "outputId": "e9f253b9-96e9-4c90-a51c-9d443de0cdba"
   },
   "outputs": [],
   "source": [
    "!pip uninstall torchtext\n",
    "!pip install torch==1.8.2+cu102 torchaudio==0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
    "!pip install pytorch-lightning psutil\n",
    "!git clone https://github.com/SJWyatt/leaf-audio-pytorch.git leaf-audio-pytorch-fork && cd leaf-audio-pytorch-fork && pip install .\n",
    "\n",
    "!pip install \"ray[tune]\"\n",
    "!pip install \"ray[default]\"\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4iOqHF8S-iZZ",
   "metadata": {
    "id": "4iOqHF8S-iZZ"
   },
   "outputs": [],
   "source": [
    "import sys, os, math, random, functools\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim import Adam\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything, loggers\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "from leaf_audio_pytorch import frontend, initializers\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.wandb import WandbLogger, wandb_mixin\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "\n",
    "# import rans\n",
    "# import util\n",
    "# from torch_vae.tvae_beta_binomial import BetaBinomialVAE\n",
    "# from torch_vae import tvae_utils\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"Bosch_Audio\")\n",
    "losses = {k:v for k, v in vars(torch.nn.modules.loss).items() if k.endswith('Loss')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh8eTMTN5mEK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vh8eTMTN5mEK",
    "outputId": "69486fc3-87ed-4991-ba4b-abb3c6e933eb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")\n",
    "# !cd /content/gdrive/MyDrive\n",
    "\n",
    "# !cp /content/gdrive/MyDrive/rans.py .\n",
    "# !cp /content/gdrive/MyDrive/util.py .\n",
    "# !cp -avr /content/gdrive/MyDrive/torch_vae .\n",
    "\n",
    "# # LARGE DATASET\n",
    "# !gdown --id 1KSr-GF8avGTnPA83AVE3Hjz4TjWRlnpc\n",
    "# !mkdir UrbanSound8KZIP\n",
    "# !tar -xvf  'UrbanSound8K.tar.gz' -C '/content/UrbanSound8KZIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hqhzdf4B59MS",
   "metadata": {
    "id": "hqhzdf4B59MS"
   },
   "outputs": [],
   "source": [
    "# LARGE DATASET\n",
    "# dataset_folder = \"/content/UrbanSound8KZIP/UrbanSound8K\"\n",
    "\n",
    "# LOCAL DATASET\n",
    "dataset_folder = \"/home/dustedduke/Documents/Bosch_Audio_Project_Clean/data/UrbanSound8K\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7657f04d-be65-4389-8d94-ae4259af58c4",
   "metadata": {
    "id": "7657f04d-be65-4389-8d94-ae4259af58c4"
   },
   "source": [
    "## Audio transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe3209-b8b5-4466-9d08-9a3121f46ecc",
   "metadata": {
    "id": "1bbe3209-b8b5-4466-9d08-9a3121f46ecc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "class AudioUtil():\n",
    "\n",
    "    @staticmethod\n",
    "    def get_available_transforms():\n",
    "      method_list = []\n",
    "      for attribute in dir(AudioUtil):\n",
    "          attribute_value = getattr(AudioUtil, attribute)\n",
    "          if callable(attribute_value):\n",
    "              if attribute.startswith('__') == False:\n",
    "                  method_list.append(attribute)\n",
    "      method_list.remove('open')\n",
    "      method_list.remove('get_available_transforms')\n",
    "      return method_list\n",
    "\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convert the given audio to the desired number of channels\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channels):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sig.shape[0] == new_channels):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "\n",
    "        if (new_channels == 1):\n",
    "            # Convert from stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # Convert from mono to stereo by duplicating the first channel\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Since Resample applies to a single channel, we resample one channel at a time\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            # Resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "      sig, sr = aud\n",
    "      num_rows, sig_len = sig.shape\n",
    "      max_len = sr//1000 * max_ms\n",
    "\n",
    "      if (sig_len > max_len):\n",
    "        # Truncate the signal to the given length\n",
    "        sig = sig[:,:max_len]\n",
    "\n",
    "      elif (sig_len < max_len):\n",
    "        # Length of padding to add at the beginning and end of the signal\n",
    "        pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "        pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "        # Pad with 0s\n",
    "        pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "        pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "        sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        \n",
    "      return (sig, sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Shifts the signal to the left or right by some percent. Values at the end\n",
    "    # are 'wrapped around' to the start of the transformed signal.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, sr, shift_limit):\n",
    "        sig,sr = aud, sr\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "\n",
    "    # TODO fix\n",
    "    @staticmethod\n",
    "    def rans_compress(aud, config):\n",
    "      \n",
    "        # print('\\nTEST 22\\n')\n",
    "\n",
    "        image = aud[0].float().view(1, -1)\n",
    "\n",
    "        rng = np.random.RandomState(0)\n",
    "        other_bits = rng.randint(low=1 << 16, high=1 << 31, size=50, dtype=np.uint32)\n",
    "        state = rans.unflatten(other_bits)\n",
    "        state = config['vae_append'](state, image)\n",
    "\n",
    "        compressed_length = 32 * (len(rans.flatten(state)) - len(other_bits))\n",
    "        compressed_message = rans.flatten(state)\n",
    "        \n",
    "        return (torch.from_numpy(compressed_message), aud[1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def rans_decompress(aud, config):\n",
    "        scale_bits = 8\n",
    "        x = rans.unflatten(aud[0].numpy())\n",
    "        for start, freq in reversed(list(zip(config['starts'], config['freqs']))):\n",
    "            def statfun(cf):\n",
    "                assert start <= cf < start + freq\n",
    "                return None, (start, freq)\n",
    "            x, symbol = rans.pop(x, statfun, scale_bits)\n",
    "        assert x == (rans.head_min, ())\n",
    "\n",
    "        return (torch.from_numpy(aud[0]), aud[1])\n",
    "\n",
    "    def original_embedding(aud, config):\n",
    "        reaud = AudioUtil.resample(aud, 44100)\n",
    "        rechan = AudioUtil.rechannel(reaud, 2)\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, 4000)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, 0.4)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return (aug_sgram)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaf_embedding(aud, config):\n",
    "\n",
    "        # Prepare\n",
    "        reaud = AudioUtil.resample(aud, config['new_sample_rate'])\n",
    "        rechan = AudioUtil.rechannel(reaud, config['new_channels'])\n",
    "        padded = AudioUtil.pad_trunc(rechan, config['new_length'])\n",
    "\n",
    "        complex_conv_init = initializers.GaborInit(sample_rate=padded[1], min_freq=config['min_freq'], \n",
    "                                                   max_freq=config['max_freq'])\n",
    "\n",
    "        # LEAF shape [channels, time, n_filters]\n",
    "        custom_leaf = frontend.Leaf(learn_pooling=config['learn_pooling'],\n",
    "                                    n_filters=config['n_filters'],\n",
    "                                    window_len=config['window_len'],\n",
    "                                    sample_rate=padded[1],\n",
    "                                    preemp=config['preemp'],\n",
    "                                    complex_conv_init=complex_conv_init)\n",
    "\n",
    "        leaf_repr_torch = custom_leaf(padded[0])\n",
    "        leaf_repr_torch_perm = leaf_repr_torch.permute(2,1,0)\n",
    "        leaf_repr_torch_flat = leaf_repr_torch_perm.flatten(start_dim=1).detach()\n",
    "        return leaf_repr_torch_flat\n",
    "        \n",
    "    @staticmethod\n",
    "    def leaf_bits_back(aud, config):\n",
    "        # Postponed\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb9651-e672-4fc6-bb9d-0b75eaeeefaa",
   "metadata": {
    "id": "62cb9651-e672-4fc6-bb9d-0b75eaeeefaa"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcf699-7ca5-48dc-b427-3dceb4a46a00",
   "metadata": {
    "id": "d3bcf699-7ca5-48dc-b427-3dceb4a46a00"
   },
   "outputs": [],
   "source": [
    "def one_hot(idx, num_items):\n",
    "    return [(0.0 if n != idx else 1.0) for n in range(num_items)]\n",
    "\n",
    "class UrbanDataset(Dataset):\n",
    "\n",
    "    def __init__(self, config, dataset_folder, fold, transform=None, augment=None):\n",
    "        super().__init__()\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.path_to_csv = os.path.join(self.dataset_folder, \"metadata/UrbanSound8K.csv\")\n",
    "        self.path_to_audio_folder = os.path.join(self.dataset_folder, \"audio\")\n",
    "        self.metadata = pd.read_csv(self.path_to_csv) #[:2000]\n",
    "        self.fold = fold\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.config = config\n",
    "\n",
    "    def train_validation_split(self):\n",
    "        train_idx = list(self.metadata[self.metadata[\"fold\"] != self.fold].index)\n",
    "        val_idx = list(self.metadata[self.metadata[\"fold\"] == self.fold].index)\n",
    "        \n",
    "        train_set = Subset(self, train_idx)\n",
    "        val_set = Subset(self, val_idx)\n",
    "        return train_set, val_set\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        file_name = self.metadata[\"slice_file_name\"].iloc[index]\n",
    "        file_path = os.path.join(\n",
    "            os.path.join(self.path_to_audio_folder, \"fold\" + str(self.metadata[\"fold\"].iloc[index])), file_name\n",
    "        )\n",
    "\n",
    "        # Both training and validation are transformed\n",
    "        aud = torchaudio.load(file_path)\n",
    "        aud = self.transform(aud, self.config)\n",
    "        label = torch.from_numpy(np.array(one_hot(self.metadata[\"classID\"].iloc[index], 10)))\n",
    "\n",
    "        return {\n",
    "            \"input_vector\": aud,\n",
    "            \"label\": label,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2e04cd-f805-4703-b012-6b116c6113dd",
   "metadata": {
    "id": "da2e04cd-f805-4703-b012-6b116c6113dd"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc05ce-2679-4923-8cd1-e31d91f64cc9",
   "metadata": {
    "id": "c2fc05ce-2679-4923-8cd1-e31d91f64cc9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, auc, precision_recall_curve\n",
    "\n",
    "def auprc(y_true, y_scores):\n",
    "    \"\"\" Compute AUPRC for 1 class\n",
    "        Args:\n",
    "            y_true (np.array): one hot encoded labels\n",
    "            y_scores (np.array): model prediction\n",
    "        Return:\n",
    "            auc (float): the Area Under the Recall Precision curve\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    recall = np.concatenate((np.array([1.0]), recall, np.array([0.0])))\n",
    "    precision = np.concatenate((np.array([0.0]), precision, np.array([1.0])))\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def compute_macro_auprc(y_true, y_scores, return_auprc_per_class=False):\n",
    "    \"\"\" Compute macro AUPRC\n",
    "        Args:\n",
    "            y_true (np.array): one hot encoded labels\n",
    "            y_scores (np.array): model prediction\n",
    "        Return:\n",
    "            auprc_macro (float): the macro AUPRC\n",
    "    \"\"\"\n",
    "    _, num_classes = y_true.shape\n",
    "    auprc_scores = [auprc(y_true[:, i], y_scores[:, i]) for i in range(num_classes)]\n",
    "    auprc_macro = np.nanmean(np.array(auprc_scores))\n",
    "    if return_auprc_per_class:\n",
    "        return auprc_scores, auprc_macro\n",
    "    else:\n",
    "        return auprc_macro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J5ZiRXSqRbWu",
   "metadata": {
    "id": "J5ZiRXSqRbWu"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8OlyD7NeRVul",
   "metadata": {
    "id": "8OlyD7NeRVul"
   },
   "outputs": [],
   "source": [
    "class VanillaClassifier(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(VanillaClassifier, self).__init__()\n",
    "        self.config = config\n",
    "        self.fold = config['FOLD_INDEX']\n",
    "\n",
    "        self.dataset_folder = config['dataset_folder']\n",
    "        self.dropout = None\n",
    "        self.output_size = config['output_size']\n",
    "        self.embedding_dim = config['new_channels'] * config['n_filters']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.drop_prob = config['drop_prob']\n",
    "        self.loss_type = config['loss_type']\n",
    "        self.transform = config['transform']\n",
    "        self.augment = config['augment']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.shuffle = config['shuffle']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        self.previous_hidden = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "        self.previous_cell = torch.randn(self.n_layers, self.batch_size, self.hidden_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim, \n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=self.n_layers, \n",
    "            batch_first=True, \n",
    "            dropout=self.drop_prob if config['n_layers'] > 1 else 0,\n",
    "            bidirectional=False)\n",
    "        if (config['n_layers'] == 1):\n",
    "            self.dropout = nn.Dropout(config['drop_prob'])\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.float()\n",
    "        x, hidden = self.lstm(x, (self.previous_hidden, self.previous_cell))\n",
    "        self.previous_cell = hidden[0].detach()\n",
    "        self.previous_hidden = hidden[1].detach()\n",
    "\n",
    "        x = x[:, -1]\n",
    "        if (self.n_layers > 1):\n",
    "            x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "  \n",
    "    def prepare_data(self):\n",
    "          data_param = {\n",
    "              \"config\": self.config,\n",
    "              \"dataset_folder\": self.dataset_folder,\n",
    "              \"fold\": self.config['FOLD_INDEX'],\n",
    "              \"transform\": self.transform,\n",
    "              \"augment\": self.augment\n",
    "          }\n",
    "        \n",
    "          self.dataset = UrbanDataset(**data_param)\n",
    "          (self.train_dataset, self.val_dataset) = self.dataset.train_validation_split()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=2, drop_last=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, drop_last=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=2, drop_last=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "\n",
    "    def torch_accuracy(self, logits, labels):\n",
    "        predicted = torch.argmax(logits, 1)\n",
    "        target = torch.argmax(labels, 1)\n",
    "\n",
    "        correct = (predicted == target).sum().item()\n",
    "        accuracy = correct / len(target)\n",
    "        return torch.tensor(accuracy)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        data, target = (\n",
    "            batch[\"input_vector\"].float(),\n",
    "            batch[\"label\"].float()\n",
    "        )\n",
    "\n",
    "        output = self.forward(data)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(output, torch.argmax(target, dim=1).long())\n",
    "        accuracy_sc = self.torch_accuracy(output, target)\n",
    "        \n",
    "        wandb.log({\"loss\": loss})\n",
    "        wandb.log({\"2_train/1_accuracy0.5\": accuracy_sc})\n",
    "        self.log(\"loss\", loss)\n",
    "        self.log(\"2_train/1_accuracy0.5\", accuracy_sc)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        data, target= (\n",
    "            batch[\"input_vector\"].float(),\n",
    "            batch[\"label\"].float()\n",
    "        )\n",
    "\n",
    "        output = self.forward(data)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        val_loss = criterion(output, torch.argmax(target, dim=1).long())\n",
    "        val_accuracy = self.torch_accuracy(output, target)\n",
    "        \n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        self.log(\"val_accuracy\", val_accuracy)\n",
    "\n",
    "        wandb.log({\"val_loss\": val_loss.mean(0)})\n",
    "        wandb.log({\"val_accuracy\": val_accuracy})\n",
    "        wandb.log({\"FOLD_INDEX\": self.config['FOLD_INDEX']})\n",
    "\n",
    "        return {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"output\": output.detach(),\n",
    "            \"target\": target.detach(),\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        all_outputs = torch.cat([o[\"output\"] for o in outputs], 0).cpu().numpy()\n",
    "        all_targets = torch.cat([o[\"target\"] for o in outputs], 0).cpu().numpy()\n",
    "\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "\n",
    "        self.log(\"1_loss/val_loss\", avg_loss)\n",
    "        self.log(\"2_valid/1_accuracy0.5\", avg_acc)\n",
    "        wandb.log({\"1_loss/val_loss\": avg_loss.item()})\n",
    "        wandb.log({\"2_valid/1_accuracy0.5\": avg_acc.item()})\n",
    "\n",
    "        # wandb.log({\"2_valid/1_f1_micro0.5\": f1_micro})\n",
    "        # wandb.log({\"2_valid/1_auprc_micro\": auprc_micro})\n",
    "        # wandb.log({\"2_valid/1_auprc_macro\": auprc_macro})\n",
    "        \n",
    "        predicted_values = np.argmax(all_outputs, axis=-1)\n",
    "        true_values = np.argmax(all_targets, axis=-1)\n",
    "            \n",
    "        wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
    "                                preds=predicted_values, y_true=true_values,\n",
    "                                class_names=[\"Air Conditioner\", \"Car Horn\", \"Children Playing\", \"Dog Bark\", \n",
    "                                            \"Drilling\", \"Engine Idling\", \"Gun Shot\", \"Jackhammer\", \"Siren\", \"Street Music\"])})\n",
    "\n",
    "        wandb.log({\"roc_curve\" : wandb.plot.roc_curve(true_values,\n",
    "                    all_outputs, labels=[\"Air Conditioner\", \"Car Horn\", \"Children Playing\", \"Dog Bark\", \"Drilling\", \"Engine Idling\", \"Gun Shot\", \"Jackhammer\", \"Siren\", \"Street Music\"])})\n",
    "\n",
    "        wandb.log({\"pr\" : wandb.plot.pr_curve(true_values, all_outputs,\n",
    "                 labels=[\"Air Conditioner\", \"Car Horn\", \"Children Playing\", \"Dog Bark\", \"Drilling\", \"Engine Idling\", \"Gun Shot\", \"Jackhammer\", \"Siren\", \"Street Music\"], classes_to_plot=None)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0117c36-4fe3-4da2-a460-74c42b7787d3",
   "metadata": {
    "id": "f0117c36-4fe3-4da2-a460-74c42b7787d3"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6b25b-da26-431a-8d83-dd67a9b9460f",
   "metadata": {
    "id": "24d6b25b-da26-431a-8d83-dd67a9b9460f"
   },
   "outputs": [],
   "source": [
    "@wandb_mixin\n",
    "def training(config, checkpoint_dir=None):\n",
    "    seed_everything(config['seed'])\n",
    "    model = VanillaClassifier(config)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        profiler=\"simple\",\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        limit_val_batches=config['percent_valid_examples'],\n",
    "        progress_bar_refresh_rate=0,\n",
    "        checkpoint_callback=False,\n",
    "        max_epochs=config['max_epochs'],\n",
    "        gpus=None\n",
    "        log_every_n_steps=10,\n",
    "        callbacks=[\n",
    "            TuneReportCallback(\n",
    "                {\n",
    "                  \"loss\": \"loss\",\n",
    "                  \"2_train/1_accuracy0.5\": \"2_train/1_accuracy0.5\"\n",
    "                  # \"2_valid/1_accuracy0.5\",\n",
    "                  # \"2_valid/1_f1_micro0.5\",\n",
    "                  # \"2_valid/1_auprc_micro\",\n",
    "                  # \"2_valid/1_auprc_macro\"\n",
    "                },\n",
    "                on=\"batch_end\"),\n",
    "            TuneReportCallback(\n",
    "                {\n",
    "                  \"1_loss/val_loss\": \"1_loss/val_loss\"\n",
    "                  \"2_valid/1_accuracy0.5\",\n",
    "                  # \"2_valid/1_f1_micro0.5\",\n",
    "                  # \"2_valid/1_auprc_micro\",\n",
    "                  # \"2_valid/1_auprc_macro\"\n",
    "                },\n",
    "                on=\"validation_end\"),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    trainer.fit(model)\n",
    "    \n",
    "    torch.save({\n",
    "            'FOLD_INDEX': config['FOLD_INDEX'],\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            }, \"./model_state_dict\")\n",
    "    \n",
    "    torch.save(model, \"./model + \" + str(config['FOLD_INDEX']) + \".pth\")\n",
    "    \n",
    "    return np.random.randn() * 0.1 + config[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e986c-5bd8-4fb1-8f91-ff2a766c5f73",
   "metadata": {
    "id": "f85e986c-5bd8-4fb1-8f91-ff2a766c5f73"
   },
   "source": [
    "# Configure and Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mnRiiGS7vh0M",
   "metadata": {
    "id": "mnRiiGS7vh0M"
   },
   "source": [
    "## 1. Choose list of transforms and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYsVE9YsvgZ3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYsVE9YsvgZ3",
    "outputId": "55f30084-2d4d-42f8-87b4-a61c0b3ad947"
   },
   "outputs": [],
   "source": [
    "print('AVAILABLE TRANSFORMS AND AUGMENTATIONS:', *AudioUtil.get_available_transforms(), sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F2L41Pc5nSZ6",
   "metadata": {
    "id": "F2L41Pc5nSZ6"
   },
   "outputs": [],
   "source": [
    "# VAE configuration\n",
    "# prior_precision = 8\n",
    "# obs_precision = 14\n",
    "# q_precision = 14\n",
    "\n",
    "# latent_dim = 50\n",
    "# latent_shape = (1, latent_dim)\n",
    "# model = BetaBinomialVAE(hidden_dim=200, latent_dim=latent_dim)\n",
    "# model.load_state_dict(\n",
    "#     torch.load('/content/gdrive/MyDrive/torch_vae/saved_params/torch_vae_beta_binomial_params',\n",
    "#               map_location=lambda storage, location: storage))\n",
    "# model.eval()\n",
    "\n",
    "# rec_net = tvae_utils.torch_fun_to_numpy_fun(model.encode)\n",
    "# gen_net = tvae_utils.torch_fun_to_numpy_fun(model.decode)\n",
    "\n",
    "# obs_append = tvae_utils.beta_binomial_obs_append(255, obs_precision)\n",
    "# obs_pop = tvae_utils.beta_binomial_obs_pop(255, obs_precision)\n",
    "\n",
    "# vae_append = util.vae_append(latent_shape, gen_net, rec_net, obs_append,\n",
    "#                       prior_precision, q_precision)\n",
    "\n",
    "input_size = tune.choice([8])\n",
    "experiment_config = {\n",
    "  'FOLD_INDEX': tune.grid_search(list(range(1,11))),\n",
    "  \"a\": tune.uniform(0, 1),\n",
    "  \"transform\": AudioUtil.leaf_embedding, # AudioUtil.leaf_bits_back,\n",
    "  \"vae_append\": None, # vae_append,\n",
    "  \"augment\": None,\n",
    "  \"new_sample_rate\": 44100, # tune.grid_search([44100, 48000]),\n",
    "  \"new_channels\": 1, # tune.grid_search([1, 2]),\n",
    "  \"new_length\": 4000,\n",
    "  \"min_freq\": 60,\n",
    "  \"max_freq\": 7800,\n",
    "  \"n_filters\": input_size,\n",
    "  \"window_len\": 8, # tune.grid_search([32, 64]),\n",
    "  \"preemp\": False,\n",
    "  \"learn_pooling\": False,\n",
    "  \"percent_valid_examples\": 1.0,\n",
    "  'dataset_folder': dataset_folder,\n",
    "  'seed': 42,\n",
    "  'folds': 10,\n",
    "  'max_epochs': 10,\n",
    "  'embedding_dim': input_size,\n",
    "  'hidden_dim': 8, # tune.choice([32, 64]),\n",
    "  'output_size': 10,\n",
    "  'n_layers': 1,\n",
    "  'drop_prob': 0,\n",
    "  'batch_size': 32, #t une.choice([16, 32]),\n",
    "  'learning_rate': 1e-3, # tune.choice([1e-7, 1e-3]), #1e-5,\n",
    "  'loss_type': tune.choice([nn.CrossEntropyLoss]),\n",
    "  'shuffle': False,\n",
    "  'wandb': {\n",
    "    \"project\": \"Bosch_Audio\",\n",
    "    \"api_key\": \"f7753b76352daa3550476e0166a5095eacffbdce\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed09b315-92c8-45f8-8cd3-802f604cef92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed09b315-92c8-45f8-8cd3-802f604cef92",
    "outputId": "93c57014-1017-4317-a09b-b812fd9c2183"
   },
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_cpus=4, num_gpus=0)\n",
    "\n",
    "def tune_asha():\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "          max_t=10,\n",
    "          grace_period=1,\n",
    "          reduction_factor=2)\n",
    "    \n",
    "    searcher = tune.suggest.basic_variant.BasicVariantGenerator(\n",
    "        constant_grid_search=True\n",
    "    )\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "    parameter_columns=['FOLD_INDEX', 'embedding_dim', 'hidden_dim', 'batch_size'],\n",
    "    metric_columns=[\"1_loss/val_loss\", \"loss\", \"2_train/1_accuracy0.5\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "    tune.with_parameters(\n",
    "        training),\n",
    "    resources_per_trial={\n",
    "        \"cpu\": 4,\n",
    "        \"gpu\": 0\n",
    "    },\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    config=experiment_config,\n",
    "    num_samples=1,\n",
    "    scheduler=scheduler,\n",
    "    search_alg=searcher,\n",
    "    progress_reporter=reporter,\n",
    "    callbacks=[],\n",
    "    local_dir=\"./results\",\n",
    "    stop={\"training_iteration\": 10, \"loss\": 0.001},\n",
    "    name=\"tune_asha\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    \n",
    "    df = analysis.dataframe()\n",
    "    print(df.groupby([\"config/a\"]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WikWa11oBAmA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WikWa11oBAmA",
    "outputId": "1463d1a0-8f2a-4fc7-df17-f7085f05dd42"
   },
   "outputs": [],
   "source": [
    "tune_asha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ojIKZxvQzLiX",
   "metadata": {
    "id": "ojIKZxvQzLiX"
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bosch_master.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
